{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD Scripts Kaggle\n",
    "#### Created by [licyk](https://github.com/licyk)\n",
    "\n",
    "Jupyter Notebook 仓库：[licyk/sd-webui-all-in-one](https://github.com/licyk/sd-webui-all-in-one)\n",
    "\n",
    "一个在 [Kaggle](https://www.kaggle.com) 部署 [sd-scripts](https://github.com/kohya-ss/sd-scripts) 的 Jupyter Notebook。\n",
    "\n",
    "使用时请按顺序运行笔记单元。\n",
    "\n",
    "## 提示：\n",
    "1. 可以将训练数据上传至`kaggle/input`文件夹，运行安装时将会把训练数据放置`/kaggle/dataset`文件夹中。\n",
    "2. 训练需要的模型将下载至`/kaggle/sd-scripts/sd-models`文件夹中。\n",
    "3. 推荐将训练输出的模型路径改为`kaggle/working`文件夹，方便下载。\n",
    "4. 训练代码的部分需要根据自己的需求进行更改。\n",
    "5. 推荐使用 Kaggle 的 `Save Version` 的功能运行笔记，可让 Kaggle 笔记在无人值守下保持运行，直至所有单元运行完成。\n",
    "6. 如果有 [HuggingFace](https://huggingface.co) 账号或者 [ModelScope](https://modelscope.cn) 账号，可通过填写 Token 和仓库名后实现自动上传训练好的模型，仓库需要手动创建。\n",
    "7. 进入 Kaggle 笔记后，在 Kaggle 的右侧栏可以调整 kaggle 笔记的设置，也可以上传训练集等。注意，在 Kaggle 笔记的`Session options`->`ACCELERATOR`中，需要选择`GPU T4 x 2`，才能使用 GPU 进行模型训练。\n",
    "8. [`功能初始化`](#功能初始化)和[`模型上传`](#模型上传)单元通常不需要修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 功能初始化\n",
    "1. [[下一个单元 →](#参数配置)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化功能, 不需要修改\n",
    "\n",
    "# 消息格式输出\n",
    "def echo(*args):\n",
    "    for i in args:\n",
    "        print(f\":: {i}\")\n",
    "\n",
    "\n",
    "# ARIA2\n",
    "class ARIA2:\n",
    "    WORKSPACE = \"\"\n",
    "    WORKFOLDER = \"\"\n",
    "\n",
    "\n",
    "    def __init__(self, workspace, workfolder) -> None:\n",
    "        self.WORKSPACE = workspace\n",
    "        self.WORKFOLDER = workfolder\n",
    "\n",
    "\n",
    "    # 下载器\n",
    "    def aria2(self, url, path, filename):\n",
    "        import os\n",
    "        if not os.path.exists(path + \"/\" + filename):\n",
    "            echo(f\"开始下载 {filename} ，路径: {path}/{filename}\")\n",
    "            !aria2c --console-log-level=error -c -x 16 -s 16 \"{url}\" -d \"{path}\" -o \"{filename}\"\n",
    "            if os.path.exists(path + \"/\" + filename) and not os.path.exists(path + \"/\" + filename + \".aria2\"):\n",
    "                echo(f\"{filename} 下载完成\")\n",
    "            else:\n",
    "                echo(f\"{filename} 下载中断\")\n",
    "        else:\n",
    "            if os.path.exists(path + \"/\" + filename + \".aria2\"):\n",
    "                echo(f\"开始下载 {filename} ，路径: {path}/{filename}\")\n",
    "                !aria2c --console-log-level=error -c -x 16 -s 16 \"{url}\" -d \"{path}\" -o \"{filename}\"\n",
    "                if os.path.exists(path + \"/\" + filename) and not os.path.exists(path + \"/\" + filename + \".aria2\"):\n",
    "                    echo(f\"{filename} 下载完成\")\n",
    "                else:\n",
    "                    echo(f\"{filename} 下载中断\")\n",
    "            else:\n",
    "                echo(f\"{filename} 文件已存在，路径: {path}/{filename}\")\n",
    "\n",
    "\n",
    "    # 大模型下载\n",
    "    def get_sd_model(self, url, filename):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # vae模型下载\n",
    "    def get_vae_model(self, url, filename):\n",
    "        pass\n",
    "\n",
    "\n",
    "# GIT\n",
    "class GIT:\n",
    "    WORKSPACE = \"\"\n",
    "    WORKFOLDER = \"\"\n",
    "\n",
    "\n",
    "    def __init__(self, workspace, workfolder) -> None:\n",
    "        self.WORKSPACE = workspace\n",
    "        self.WORKFOLDER = workfolder\n",
    "\n",
    "\n",
    "    # 检测要克隆的项目是否存在于指定路径\n",
    "    def exists(self, addr=None, path=None, name=None):\n",
    "        import os\n",
    "        if addr is not None:\n",
    "            if path is None and name is None:\n",
    "                path = os.getcwd() + \"/\" + addr.split(\"/\").pop().split(\".git\", 1)[0]\n",
    "            elif path is None and name is not None:\n",
    "                path = os.getcwd() + \"/\" + name\n",
    "            elif path is not None and name is None:\n",
    "                path = os.path.normpath(path) + \"/\" + addr.split(\"/\").pop().split(\".git\", 1)[0]\n",
    "\n",
    "        if os.path.exists(path):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    # 克隆项目\n",
    "    def clone(self, addr, path=None, name=None):\n",
    "        import os\n",
    "        repo = addr.split(\"/\").pop().split(\".git\", 1)[0]\n",
    "        if not self.exists(addr, path, name):\n",
    "            echo(f\"开始下载 {repo}\")\n",
    "            if path is None and name is None:\n",
    "                path = os.getcwd()\n",
    "                name = repo\n",
    "            elif path is not None and name is None:\n",
    "                name = repo\n",
    "            elif path is None and name is not None:\n",
    "                path = os.getcwd()\n",
    "            !git clone {addr} \"{path}/{name}\" --recurse-submodules\n",
    "        else:\n",
    "            echo(f\"{repo} 已存在\")\n",
    "\n",
    "\n",
    "\n",
    "# ENV\n",
    "class ENV:\n",
    "    WORKSPACE = \"\"\n",
    "    WORKFOLDER = \"\"\n",
    "\n",
    "\n",
    "    def __init__(self, workspace, workfolder) -> None:\n",
    "        self.WORKSPACE = workspace\n",
    "        self.WORKFOLDER = workfolder\n",
    "\n",
    "\n",
    "    # 准备ipynb笔记自身功能的依赖\n",
    "    def prepare_env_depend(self, use_uv=False):\n",
    "        pip_mirror = \"--index-url https://pypi.python.org/simple --find-links https://download.pytorch.org/whl/cu121/torch_stable.html\"\n",
    "\n",
    "        echo(\"安装自身组件依赖\")\n",
    "        !pip install uv {pip_mirror}\n",
    "        if use_uv:\n",
    "            !uv pip install huggingface_hub modelscope {pip_mirror} --system --quiet\n",
    "        else:\n",
    "            !pip install huggingface_hub modelscope {pip_mirror}\n",
    "        !apt update\n",
    "        !apt install aria2 google-perftools p7zip-full unzip -y\n",
    "\n",
    "\n",
    "    # 安装pytorch和xformers\n",
    "    def prepare_torch(self, torch_ver, xformers_ver, use_uv=False):\n",
    "        pip_mirror = \"--index-url https://pypi.python.org/simple --find-links https://download.pytorch.org/whl/cu121/torch_stable.html\"\n",
    "        if use_uv:\n",
    "            if torch_ver != \"\":\n",
    "                echo(\"安装 PyTorch\")\n",
    "                !uv pip install {torch_ver} {pip_mirror} --system --quiet\n",
    "            if xformers_ver != \"\":\n",
    "                echo(\"安装 xFormers\")\n",
    "                !uv pip install {xformers_ver} {pip_mirror} --no-deps --system --quiet\n",
    "        else:\n",
    "            if torch_ver != \"\":\n",
    "                echo(\"安装 PyTorch\")\n",
    "                !pip install {torch_ver} {pip_mirror}\n",
    "            if xformers_ver != \"\":\n",
    "                echo(\"安装 xFormers\")\n",
    "                !pip install {xformers_ver} {pip_mirror} --no-deps\n",
    "    \n",
    "\n",
    "    # 安装requirements.txt依赖\n",
    "    def install_requirements(self, path, use_uv=False):\n",
    "        import os\n",
    "        pip_mirror = \"--index-url https://pypi.python.org/simple --find-links https://download.pytorch.org/whl/cu121/torch_stable.html\"\n",
    "        if os.path.exists(path):\n",
    "            echo(\"安装依赖\")\n",
    "            if use_uv:\n",
    "                !uv pip install -r \"{path}\" {pip_mirror} --system --quiet\n",
    "            else:\n",
    "                !pip install -r \"{path}\" {pip_mirror}\n",
    "        else:\n",
    "            echo(\"依赖文件路径为空\")\n",
    "\n",
    "\n",
    "    # python软件包安装\n",
    "    # 可使用的操作:\n",
    "    # 安装: install -> install\n",
    "    # 仅安装: install_single -> install --no-deps\n",
    "    # 强制重装: force_install -> install --force-reinstall\n",
    "    # 仅强制重装: force_install_single -> install --force-reinstall --no-deps\n",
    "    # 更新: update -> install --upgrade\n",
    "    # 卸载: uninstall -y\n",
    "    def py_pkg_manager(self, pkg, type=None, use_uv=False):\n",
    "        pip_mirror = \"--index-url https://pypi.python.org/simple --find-links https://download.pytorch.org/whl/cu121/torch_stable.html\"\n",
    "\n",
    "        if type == \"install\":\n",
    "            func = \"install\"\n",
    "            args = \"\"\n",
    "        elif type == \"install_single\":\n",
    "            func = \"install\"\n",
    "            args = \"--no-deps\"\n",
    "        elif type == \"force_install\":\n",
    "            func = \"install\"\n",
    "            args = \"--force-reinstall\"\n",
    "        elif type == \"force_install_single\":\n",
    "            func = \"install\"\n",
    "            args = \"install --force-reinstall --no-deps\"\n",
    "        elif type == \"update\":\n",
    "            func = \"install\"\n",
    "            args = \"--upgrade\"\n",
    "        elif type == \"uninstall\":\n",
    "            func = \"uninstall\"\n",
    "            args = \"-y\"\n",
    "            pip_mirror = \"\"\n",
    "        else:\n",
    "            echo(f\"未知操作: {type}\")\n",
    "            return\n",
    "        if use_uv:\n",
    "            echo(f\"执行操作: uv pip {func} {pkg} {args} {pip_mirror} --system --quiet\")\n",
    "            !uv pip {func} {pkg} {args} {pip_mirror} --system --quiet\n",
    "        else:\n",
    "            echo(f\"执行操作: pip {func} {pkg} {args} {pip_mirror}\")\n",
    "            !pip {func} {pkg} {args} {pip_mirror}\n",
    "\n",
    "\n",
    "    # 配置内存优化\n",
    "    def tcmalloc(self):\n",
    "        echo(\"配置内存优化\")\n",
    "        import os\n",
    "        os.environ[\"LD_PRELOAD\"] = \"/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4\"\n",
    "\n",
    "\n",
    "\n",
    "# MANAGER\n",
    "class MANAGER:\n",
    "    WORKSPACE = \"\"\n",
    "    WORKFOLDER = \"\"\n",
    "\n",
    "\n",
    "    def __init__(self, workspace, workfolder) -> None:\n",
    "        self.WORKSPACE = workspace\n",
    "        self.WORKFOLDER = workfolder\n",
    "\n",
    "\n",
    "    # 清理ipynb笔记的输出\n",
    "    def clear_up(self, opt):\n",
    "        from IPython.display import clear_output\n",
    "        clear_output(wait=opt)\n",
    "\n",
    "\n",
    "    # 检查gpu是否可用\n",
    "    def check_gpu(self):\n",
    "        echo(\"检测 GPU 是否可用\")\n",
    "        import tensorflow as tf\n",
    "        echo(f\"TensorFlow 版本: {tf.__version__}\")\n",
    "        if tf.test.gpu_device_name():\n",
    "            echo(\"GPU 可用\")\n",
    "        else:\n",
    "            echo(\"GPU 不可用\")\n",
    "            raise Exception(\"\\n没有使用GPU，请在代码执行程序-更改运行时类型-设置为GPU！\\n如果不能使用GPU，建议更换账号！\")\n",
    "\n",
    "\n",
    "\n",
    "class REPO_MANAGER:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    # 获取文件夹中所有的文件的绝对路径\n",
    "    def get_all_file(self, directory):\n",
    "        import os\n",
    "        file_list = []\n",
    "        for dirname, _, filenames in os.walk(directory):\n",
    "            for filename in filenames:\n",
    "                file_list.append(os.path.join(dirname, filename))\n",
    "        return file_list\n",
    "\n",
    "\n",
    "    # ModelScope\n",
    "    # 检测是否存在于仓库中\n",
    "    def is_file_exists_in_ms_repo(self, upload_file, work_path, repo):\n",
    "        import os\n",
    "        repo_file = os.path.join(work_path, repo.split(\"/\").pop(), upload_file)\n",
    "        if os.path.exists(repo_file):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    # 克隆仓库\n",
    "    def clone_modelscope_without_lfs(self, ms_access_token, repo, work_path):\n",
    "        import os\n",
    "        # 禁用 Git LFS\n",
    "        os.environ[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
    "        !git lfs uninstall\n",
    "\n",
    "        # 本地存在仓库时进行删除\n",
    "        repo_name = repo.split(\"/\").pop()\n",
    "        path = os.path.join(work_path, repo_name)\n",
    "        if os.path.exists(path):\n",
    "            !rm -rf \"{path}\"\n",
    "\n",
    "        # 下载仓库并启用 Git LFS\n",
    "        repo_url = f\"https://oauth2:{ms_access_token}@www.modelscope.cn/{repo}.git\"\n",
    "        !git clone \"{repo_url}\" \"{path}\"\n",
    "        os.environ[\"GIT_LFS_SKIP_SMUDGE\"] = \"0\"\n",
    "        !git lfs install\n",
    "\n",
    "\n",
    "    # 上传文件至 ModelScope\n",
    "    def push_file_to_modelscope(self, ms_access_token, repo, work_path, upload_path):\n",
    "        import os\n",
    "        from modelscope.hub.api import HubApi\n",
    "        if repo.split(\"/\").pop() == upload_path.split(\"/\").pop():\n",
    "            echo(\"本地要上传的仓库名称与要上传到的仓库的名称相同, 这将导致本地要上传的仓库被自动删除\")\n",
    "            return\n",
    "\n",
    "        api = HubApi()\n",
    "        try:\n",
    "            ms_access_token = api.login(ms_access_token)[0] # 将 ModelScope Token 转为 ModelScope Git Token\n",
    "            echo(\"ModelScope Token 验证成功\")\n",
    "        except Exception as e:\n",
    "            echo(\"ModelScope Token 验证失败: \", e)\n",
    "            return\n",
    "\n",
    "        os.chdir(work_path)\n",
    "        count = 0\n",
    "        upload_file_lists = self.get_all_file(upload_path) # 原文件的路径列表\n",
    "\n",
    "        echo(f\"将文件上传至 ModelScope:: {upload_path} -> {os.path.join(work_path, repo.split('/').pop())}\")\n",
    "        count = 0\n",
    "        sum = len(upload_file_lists)\n",
    "        for upload_file in upload_file_lists:\n",
    "            count += 1\n",
    "\n",
    "            echo(f\"[{count}/{sum}]:: 克隆仓库到 {work_path}\")\n",
    "            self.clone_modelscope_without_lfs(ms_access_token, repo, work_path)\n",
    "            rel_upload_file = os.path.relpath(upload_file, upload_path) # 原文件相对路径\n",
    "            repo_path = os.path.join(work_path, repo.split(\"/\").pop()) # 仓库的绝对路径\n",
    "            echo(f\"[{count}/{sum}]:: 要上传的文件的相对路径: {rel_upload_file}\")\n",
    "            echo(f\"[{count}/{sum}]:: 绝对路径: {upload_file}\")\n",
    "            echo(f\"[{count}/{sum}]:: 仓库地址: {repo_path}\")\n",
    "\n",
    "            # 检测文件是否存在于仓库中\n",
    "            if self.is_file_exists_in_ms_repo(rel_upload_file, work_path, repo):\n",
    "                !rm -rf \"{repo_path}\"\n",
    "                echo(f\"[{count}/{sum}]:: {os.path.basename(upload_file)} 已存在于仓库中\")\n",
    "            else:\n",
    "                # 为仓库创建对应的文件夹\n",
    "                p_path = os.path.dirname(os.path.join(work_path, repo.split(\"/\").pop(), rel_upload_file)) # 原文件到仓库中后对应的父文件夹\n",
    "                if not os.path.exists(p_path):\n",
    "                    echo(f\"[{count}/{sum}]:: 创建文件对应的父文件夹: {p_path}\")\n",
    "                    os.makedirs(p_path, exist_ok=True)\n",
    "\n",
    "                echo(f\"[{count}/{sum}]:: {upload_file} -> {repo_path}\")\n",
    "                !cp \"{upload_file}\" \"{p_path}\"\n",
    "\n",
    "                os.chdir(repo_path)\n",
    "                file_name = rel_upload_file.split(\"/\").pop() # 文件名\n",
    "                echo(f\"[{count}/{sum}]:: 添加文件： {rel_upload_file}\")\n",
    "                !git add \"{rel_upload_file}\"\n",
    "                echo(f\"[{count}/{sum}]:: 提交信息: \\\"upload {file_name}\\\"\")\n",
    "                !git commit -m \"upload {file_name}\"\n",
    "                !git config lfs.https://oauth2:{ms_access_token}@www.modelscope.cn/{repo}.git/info/lfs.locksverify true\n",
    "                echo(f\"[{count}/{sum}]:: 上传 {file_name} 到 {repo} 中\")\n",
    "                !git push\n",
    "\n",
    "                os.chdir(work_path)\n",
    "                !rm -rf \"{repo_path}\"\n",
    "                echo(f\"[{count}/{sum}]:: 上传 {file_name} 完成\")\n",
    "\n",
    "        echo(f\"[{count}/{sum}]:: {repo} 仓库上传完成\")\n",
    "\n",
    "\n",
    "    # HuggingFace\n",
    "    # 获取 HuggingFace 仓库中的文件列表\n",
    "    def get_hf_repo_file_list(self, hf_access_token, repo, repo_type):\n",
    "        from huggingface_hub import HfApi\n",
    "        api = HfApi()\n",
    "        model_list = api.list_repo_files(\n",
    "            repo_id = repo,\n",
    "            repo_type = repo_type,\n",
    "            token = hf_access_token\n",
    "        )\n",
    "        return model_list\n",
    "\n",
    "\n",
    "    # 上传文件到 HuggingFace\n",
    "    def push_file_to_huggingface(self, hf_access_token, repo, repo_type, work_path, upload_path):\n",
    "        import os\n",
    "        from pathlib import Path\n",
    "        from huggingface_hub import HfApi, CommitOperationAdd\n",
    "        api = HfApi()\n",
    "\n",
    "        try:\n",
    "            api.whoami(token = hf_access_token)\n",
    "            echo(\"HuggingFace Token 验证成功\")\n",
    "        except Exception as e:\n",
    "            echo(\"HuggingFace Token 验证失败: \", e)\n",
    "\n",
    "        os.chdir(work_path)\n",
    "        count = 0\n",
    "        upload_file_lists = self.get_all_file(upload_path) # 原文件的路径列表\n",
    "\n",
    "        echo(f\"将文件上传至 HuggingFace:: {upload_path} -> {os.path.join(work_path, repo.split('/').pop())}\")\n",
    "        count = 0\n",
    "        sum = len(upload_file_lists)\n",
    "        hf_repo_flie_list = self.get_hf_repo_file_list(hf_access_token, repo, repo_type) # 获取仓库中的文件列表\n",
    "        for upload_file in upload_file_lists:\n",
    "            count += 1\n",
    "\n",
    "            echo(f\"[{count}/{sum}]:: 克隆仓库到 {work_path}\")\n",
    "            rel_upload_file = os.path.relpath(upload_file, upload_path) # 原文件相对路径\n",
    "            repo_path = os.path.join(work_path, repo.split(\"/\").pop()) # 仓库的绝对路径\n",
    "            echo(f\"[{count}/{sum}]:: 要上传的文件的相对路径: {rel_upload_file}\")\n",
    "            echo(f\"[{count}/{sum}]:: 绝对路径: {upload_file}\")\n",
    "            echo(f\"[{count}/{sum}]:: 仓库地址: {repo_path}\")\n",
    "\n",
    "            # 检测文件是否存在于仓库中\n",
    "            if rel_upload_file in hf_repo_flie_list:\n",
    "                echo(f\"[{count}/{sum}]:: {os.path.basename(upload_file)} 已存在于仓库中\")\n",
    "            else:\n",
    "                file_name = rel_upload_file.split(\"/\").pop() # 文件名\n",
    "                hf_path_in_repo = Path(rel_upload_file).as_posix()\n",
    "                local_file_obj = Path(upload_file).as_posix()\n",
    "                echo(f\"[{count}/{sum}]:: {local_file_obj} -> {repo}/{hf_path_in_repo}\")\n",
    "                operations = [ CommitOperationAdd(path_in_repo = hf_path_in_repo, path_or_fileobj = local_file_obj) ]\n",
    "                echo(f\"[{count}/{sum}]:: 上传 {file_name} 到 {repo} 中\")\n",
    "                try:\n",
    "                    api.create_commit(\n",
    "                        repo_id = repo,\n",
    "                        operations = operations,\n",
    "                        commit_message = f\"Upload {file_name}\",\n",
    "                        repo_type = repo_type,\n",
    "                        token = hf_access_token\n",
    "                    )\n",
    "                    echo(f\"[{count}/{sum}]:: 上传 {file_name} 完成\")\n",
    "                except:\n",
    "                    echo(f\"[{count}/{sum}]:: 上传 {file_name} 失败\")\n",
    "\n",
    "        echo(f\"[{count}/{sum}]:: {repo} 仓库上传完成\")\n",
    "\n",
    "\n",
    "    # HuggingFace / ModelScope Token 验证\n",
    "    def verify_huggingface_token(self, hf_token):\n",
    "        from huggingface_hub import HfApi\n",
    "        api = HfApi()\n",
    "        try:\n",
    "            api.whoami(token = hf_token)\n",
    "            echo(\"HuggingFace Token 验证成功\")\n",
    "        except Exception as e:\n",
    "            echo(\"HuggingFace Token 验证失败: \", e)\n",
    "\n",
    "\n",
    "    def verify_modelscope_token(self, ms_token):\n",
    "        from modelscope.hub.api import HubApi\n",
    "        api = HubApi()\n",
    "        try:\n",
    "            api.login(ms_token)\n",
    "            echo(\"ModelScope Token 验证成功\")\n",
    "        except Exception as e:\n",
    "            echo(\"ModelScope Token 验证失败: \", e)\n",
    "\n",
    "\n",
    "    def get_modelscope_git_token(ms_token):\n",
    "        from modelscope.hub.api import HubApi\n",
    "        api = HubApi()\n",
    "        try:\n",
    "            token = api.login(ms_token)[0]\n",
    "            return token\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "    # 配置 Git 信息\n",
    "    def set_git_config(self, email, username):\n",
    "        echo(\"配置 Git 信息中\")\n",
    "        !git config --global user.email \"{email}\"\n",
    "        !git config --global user.name \"{username}\"\n",
    "\n",
    "\n",
    "\n",
    "class DATASET(ARIA2):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "    def get_dataset(self, dataset_path, url, name = None):\n",
    "        import os\n",
    "        path = \"/tmp\"\n",
    "        if name is None:\n",
    "            name = url.split(\"/\").pop()\n",
    "        super().aria2(url, path, name) # 下载文件\n",
    "        archive_format = name.split(\".\").pop()\n",
    "        origin_file_path = os.path.join(path, name)\n",
    "        echo(f\"尝试解压 {name}\")\n",
    "        if archive_format == \"7z\":\n",
    "            !7z x \"{origin_file_path}\" -o\"{dataset_path}\"\n",
    "        elif archive_format == \"zip\":\n",
    "            !unzip \"{origin_file_path}\" -d \"{dataset_path}\"\n",
    "        elif archive_format == \"tar\":\n",
    "            !tar -xvf \"{origin_file_path}\" -C \"{dataset_path}\"\n",
    "        else:\n",
    "            echo(f\"{name} 的格式不支持解压\")\n",
    "\n",
    "\n",
    "\n",
    "# SD_SCRIPTS\n",
    "class SD_SCRIPTS(ARIA2, GIT, MANAGER, ENV):\n",
    "    WORKSPACE = \"\"\n",
    "    WORKFOLDER = \"\"\n",
    "\n",
    "    repo_manager = REPO_MANAGER()\n",
    "    dataset = DATASET()\n",
    "\n",
    "    def __init__(self, workspace, workfolder) -> None:\n",
    "        self.WORKSPACE = workspace\n",
    "        self.WORKFOLDER = workfolder\n",
    "\n",
    "\n",
    "    def get_sd_model(self, url, filename = None):\n",
    "        path = self.WORKSPACE + \"/\" + self.WORKFOLDER + \"/sd-models\"\n",
    "        filename = url.split(\"/\").pop() if filename is None else filename\n",
    "        super().aria2(url, path, filename)\n",
    "\n",
    "\n",
    "    def get_vae_model(self, url, filename = None):\n",
    "        path = self.WORKSPACE + \"/\" + self.WORKFOLDER + \"/sd-models\"\n",
    "        filename = url.split(\"/\").pop() if filename is None else filename\n",
    "        super().aria2(url, path, filename)\n",
    "\n",
    "\n",
    "    def get_sd_model_from_list(self, list):\n",
    "        for i in list:\n",
    "            if i != \"\":\n",
    "                self.get_sd_model(i, i.split(\"/\").pop())\n",
    "\n",
    "\n",
    "    def get_vae_model_from_list(self, list):\n",
    "        for i in list:\n",
    "            if i != \"\":\n",
    "                self.get_vae_model(i, i.split(\"/\").pop())\n",
    "\n",
    "\n",
    "    def install(self, torch_ver, xformers_ver, sd, vae, use_uv):\n",
    "        import os\n",
    "        self.check_gpu()\n",
    "        self.prepare_env_depend(use_uv)\n",
    "        self.clone(\"https://github.com/kohya-ss/sd-scripts\", self.WORKSPACE)\n",
    "        os.chdir(os.path.join(self.WORKSPACE, self.WORKFOLDER))\n",
    "        self.prepare_torch(torch_ver, xformers_ver, use_uv)\n",
    "        req_file = os.path.join(self.WORKSPACE, self.WORKFOLDER, \"requirements.txt\")\n",
    "        self.install_requirements(req_file, use_uv)\n",
    "        self.py_pkg_manager(\"lycoris-lora dadaptation open-clip-torch wandb\", \"install\", use_uv)\n",
    "        self.prepare_torch(torch_ver, xformers_ver, use_uv)\n",
    "        self.tcmalloc()\n",
    "        self.get_sd_model_from_list(sd)\n",
    "        self.get_vae_model_from_list(vae)\n",
    "\n",
    "###################################\n",
    "echo(\"初始化功能完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数配置\n",
    "2. [[← 上一个单元](#功能初始化)|[下一个单元 →](#安装)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境设置\n",
    "WORKSPACE = \"/kaggle\" # 工作路径, 通常不需要修改\n",
    "WORKFOLDER = \"sd-scripts\" # 工作路径中文件夹名称, 通常不需要修改\n",
    "TORCH_VER = \"torch==2.3.0+cu121 torchvision==0.18.0+cu121 torchaudio==2.3.0+cu121\" # PyTorch 版本\n",
    "XFORMERS_VER = \"xformers==0.0.26.post1\" # xFormers 版本\n",
    "USE_UV = True # 使用 uv 加速 Python 软件包安装\n",
    "\n",
    "\n",
    "\n",
    "# 模型上传设置, 使用 HuggingFace / ModelScope 上传训练好的模型\n",
    "# HuggingFace: https://huggingface.co\n",
    "# ModelScope: https://modelscope.cn\n",
    "USE_HF_TO_SAVE_MODEL = False # 使用 HuggingFace 保存训练好的模型, 修改为 True 为启用, False 为禁用\n",
    "USE_MS_TO_SAVE_MODEL = False # 使用 ModelScope 保存训练好的模型, 修改为 True 为启用, False 为禁用\n",
    "\n",
    "# HuggingFace Token 在 Account -> Settings -> Access Tokens 中获取\n",
    "HF_TOKEN = \"\" # HuggingFace Token\n",
    "# ModelScope Token 在 首页 -> 访问令牌 -> SDK 令牌 中获取\n",
    "MS_TOKEN = \"\" # ModelScope Token\n",
    "\n",
    "# HuggingFace / ModelScope 模型仓库的 ID, 需要在 HuggingFace / ModelScope 上提前新建一个模型仓库\n",
    "HF_REPO_ID = \"username/reponame\" # HuggingFace 仓库的 ID\n",
    "HF_REPO_TYPE = \"model\" # HuggingFace 仓库的种类(model / dataset / space), 如果在 HuggingFace 新建的仓库为模型仓库则不需要修改\n",
    "MS_REPO_ID = \"username/reponame\" # ModelScope 仓库的 ID\n",
    "\n",
    "# Git 信息设置, 用于上传模型至 ModelScope 时使用, 可以使用默认值\n",
    "GIT_USER_EMAIL = \"username@xxx.com\" # Git 的邮箱\n",
    "GIT_USER_NAME = \"username\" # Git 的用户名\n",
    "\n",
    "\n",
    "\n",
    "# 路径设置, 通常保持默认即可\n",
    "INPUT_DATASET_PATH = \"/kaggle/dataset\" # 训练集保存的路径\n",
    "OUTPUT_PATH = \"/kaggle/working/model\" # 训练时模型保存的路径\n",
    "SD_MODEL_PATH = \"/kaggle/sd-scripts/sd-models\" # 模型下载到的路径\n",
    "KAGGLE_INPUT_PATH = \"/kaggle/input\" # Kaggle Input 的路径\n",
    "\n",
    "\n",
    "\n",
    "# 训练模型设置, 在安装时将会下载选择的模型\n",
    "# 前面的为模型的下载链接, 后面为是否下载模型的标记, 如果设置为 1 则会下载, 如果设置为 0 则不下载\n",
    "# 下载链接中最后一个斜杠后的内容作为模型下载后的保存的名称, 如:\n",
    "# https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/v1-5-pruned-emaonly.safetensors, 则下载后模型保存的名称为 v1-5-pruned-emaonly.safetensors\n",
    "\n",
    "# Stable Diffusion 模型\n",
    "SD_MODEL = [\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/v1-5-pruned-emaonly.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/animefull-final-pruned.safetensors\", 1],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/Counterfeit-V3.0_fp16.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/cetusMix_Whalefall2.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/cuteyukimixAdorable_neochapter3.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/ekmix-pastel-fp16-no-ema.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/ex2K_sse2.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/kohakuV5_rev2.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/meinamix_meinaV11.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/oukaStar_10.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/pastelMixStylizedAnime_pastelMixPrunedFP16.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/rabbit_v6.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/sweetSugarSyndrome_rev15.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/AnythingV5Ink_ink.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/bartstyledbBlueArchiveArtStyleFineTunedModel_v10.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/meinapastel_v6Pastel.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/qteamixQ_omegaFp16.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/tmndMix_tmndMixSPRAINBOW.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/sd_xl_base_1.0_0.9vae.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/animagine-xl-3.0.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/AnythingXL_xl.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/abyssorangeXLElse_v10.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/animaPencilXL_v200.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/animagine-xl-3.1.safetensors\", 1],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/heartOfAppleXL_v20.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/baxlBartstylexlBlueArchiveFlatCelluloid_xlv1.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/kohaku-xl-delta-rev1.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/kohakuXLEpsilon_rev1.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/kohaku-xl-epsilon-rev3.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/kohaku-xl-zeta.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/nekorayxl_v06W3.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/CounterfeitXL-V1.0.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/ponyDiffusionV6XL_v6StartWithThisOne.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/Illustrious-XL-v0.1.safetensors\", 1]\n",
    "]\n",
    "\n",
    "# VAE 模型\n",
    "VAE = [\n",
    "    [\"https://huggingface.co/licyk/sd-vae/resolve/main/sd_1.5/vae-ft-ema-560000-ema-pruned.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-vae/resolve/main/sd_1.5/vae-ft-mse-840000-ema-pruned.safetensors\", 1],\n",
    "    [\"https://huggingface.co/licyk/sd-vae/resolve/main/sdxl_1.0/sdxl_fp16_fix_vae.safetensors\", 1]\n",
    "]\n",
    "\n",
    "\n",
    "##########################\n",
    "echo(\"参数设置完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安装\n",
    "3. [[← 上一个单元](#参数配置)|[下一个单元 →](#模型训练)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(WORKSPACE)\n",
    "sd_scripts = SD_SCRIPTS(WORKSPACE, WORKFOLDER)\n",
    "\n",
    "\n",
    "\n",
    "# 处理模型列表\n",
    "sd_model_list = []\n",
    "vae_list = []\n",
    "for i in SD_MODEL:\n",
    "    if i[1] == 1:\n",
    "        sd_model_list.append(i[0])\n",
    "\n",
    "for i in VAE:\n",
    "    if i[1] == 1:\n",
    "        vae_list.append(i[0])\n",
    "\n",
    "\n",
    "\n",
    "echo(f\"开始安装 sd-scripts\")\n",
    "sd_scripts.install(TORCH_VER, XFORMERS_VER, sd_model_list, vae_list, USE_UV) # 安装 sd-scripts\n",
    "os.makedirs(os.path.join(WORKSPACE, \"working\", \"MS_REPO\"), exist_ok = True) # 为 ModelScope 仓库创建临时文件夹\n",
    "os.makedirs(OUTPUT_PATH, exist_ok = True) # 创建训练时保存模型的路径(由 OUTPUT_PATH 变量设定)\n",
    "os.makedirs(INPUT_DATASET_PATH, exist_ok = True) # 创建存放训练集的路径(由 INPUT_DATASET_PATH 变量设定)\n",
    "\n",
    "\n",
    "\n",
    "# 将 KAGGLE_INPUT_PATH 内的文件移动到 INPUT_DATASET_PATH 指定的路径\n",
    "if os.path.exists(KAGGLE_INPUT_PATH):\n",
    "    for i in os.listdir(KAGGLE_INPUT_PATH):\n",
    "        f = os.path.join(KAGGLE_INPUT_PATH, i)\n",
    "        !cp -r \"{f}\" \"{INPUT_DATASET_PATH}\"\n",
    "\n",
    "\n",
    "\n",
    "# 如果需要安装某个软件包, 或者切换 sd-scripts 的版本或者分支, 参考下面的写法\n",
    "# 选中代码后可以使用 Ctrl + / 取消注释\n",
    "# sd_scripts.py_pkg_manager(\"lycoris-lora==2.1.0.post3 dadaptation==3.1\", \"install\")\n",
    "# sd_scripts_path = os.path.join(WORKSPACE, WORKFOLDER)\n",
    "# !git -C \"{sd_scripts_path}\" reset --hard f8f5b16\n",
    "# !git -C \"{sd_scripts_path}\" checkout dev\n",
    "\n",
    "\n",
    "# 下载训练集压缩包并解压\n",
    "# 压缩包格式仅支持 7z, zip, tar\n",
    "# 使用格式:\n",
    "# sd_scripts.dataset.get_dataset(INPUT_DATASET_PATH, \"https://modelscope.cn/models/user/repo/resolve/master/data_1.7z\")\n",
    "# sd_scripts.dataset.get_dataset(INPUT_DATASET_PATH, \"https://modelscope.cn/models/user/repo/resolve/master/data_1.7z\", \"data_1.7z\")\n",
    "# \n",
    "# 训练集的要求:\n",
    "# 需要将图片进行打标, 并调整训练集为指定的目结构, 例如:\n",
    "# Nachoneko\n",
    "#     └── 1_nachoneko\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.png\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.txt\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).png\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).txt\n",
    "#             ├── 0(8).txt\n",
    "#             ├── 0(8).webp\n",
    "#             ├── 001_2.png\n",
    "#             ├── 001_2.txt\n",
    "#             ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.png\n",
    "#             ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.txt\n",
    "#             ├── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.png\n",
    "#             └── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.txt\n",
    "# \n",
    "# 在 Nachoneko 文件夹新建一个文件夹, 格式为 <数字>_<名称>, 如 1_nachoneko, 前面的数字代表这部分的训练集的重复次数, 1_nachoneko 文件夹内则放图片和打标文件\n",
    "# \n",
    "# 训练集也可以分成多个部分组成, 例如:\n",
    "# Nachoneko\n",
    "#     ├── 1_nachoneko\n",
    "#     │       ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.png\n",
    "#     │       ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.txt\n",
    "#     │       ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).png\n",
    "#     │       └── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).txt\n",
    "#     ├── 2_nachoneko\n",
    "#     │       ├── 0(8).txt\n",
    "#     │       ├── 0(8).webp\n",
    "#     │       ├── 001_2.png\n",
    "#     │       └── 001_2.txt\n",
    "#     └── 4_nachoneko\n",
    "#             ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.png\n",
    "#             ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.txt\n",
    "#             ├── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.png\n",
    "#             └── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.txt\n",
    "# \n",
    "# 处理好训练集并调整好目录结构后可以将 Nachoneko 文件夹进行压缩了, 使用 zip / 7z / tar 格式进行压缩\n",
    "# 例如将上述的训练集压缩成 Nachoneko.7z, 此时需要检查一下压缩后在压缩包的目录结果是否和原来的一致(有些压缩软件在部分情况下会破坏原来的目录结构)\n",
    "# 确认没有问题后将该训练集上传到网盘, 推荐使用 HuggingFace / ModelScope\n",
    "# \n",
    "# 如果不需要该功能向 Kaggle 传入数据集, 可使用 Kaggle 的 DataSet 功能\n",
    "# 在安装时 Kaggle 的 Dataset 中的训练集将会复制到 INPUT_DATASET_PATH 的目录中\n",
    "\n",
    "\n",
    "\n",
    "# 如果需要下载自己的模型, 可以参考下面的写法\n",
    "# sd_scripts.get_sd_model(\"https://civitai.com/api/download/models/889818?type=Model&format=SafeTensor&size=pruned&fp=fp16\", \"Illustrious-XL-v0.1.safetensors\")\n",
    "# sd_scripts.get_sd_model(\"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0/resolve/main/Illustrious-XL-v0.1.safetensors\")\n",
    "# 模型将会下载到 /kaggle/sd-scripts/sd-models\n",
    "\n",
    "\n",
    "\n",
    "sd_scripts.clear_up(False)\n",
    "echo(\"sd-scripts 安装完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练\n",
    "需自行编写命令，下方有可参考的例子  \n",
    "4. [[← 上一个单元](#安装)|[下一个单元 →](#模型上传)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(WORKSPACE, WORKFOLDER))\n",
    "##########################################################################################\n",
    "# 1.\n",
    "# 运行前需要根据自己的需求更改参数\n",
    "# \n",
    "# 训练参数的设置可参考：\n",
    "# https://rentry.org/59xed3\n",
    "# https://github.com/kohya-ss/sd-scripts?tab=readme-ov-file#links-to-usage-documentation\n",
    "# https://github.com/bmaltais/kohya_ss/wiki/LoRA-training-parameters\n",
    "# \n",
    "# \n",
    "# 2.\n",
    "# 下方被注释的代码选择后使用 Ctrl + / 取消注释\n",
    "# \n",
    "# \n",
    "# 3.\n",
    "# 训练使用的底模会被下载到 {SD_MODEL_PATH}, 即 /kaggle/sd-scripts/sd-models\n",
    "# 填写底模路径时一般可以通过 --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/base_model.safetensors\" 指定\n",
    "# \n",
    "# 通过 Kaggle DataSet 导入的训练集保存在 /kaggle/input, 运行该笔记时将会把训练集复制进 /kaggle/dataset\n",
    "# 该路径可通过 INPUT_DATASET_PATH 调整\n",
    "# 如果使用 sd_scripts.dataset.get_dataset() 函数下载训练集, 数据集一般会解压到 /kaggle/dataset, 这取决于函数第一个参数传入的路径\n",
    "# 训练集的路径通常要这种结构\n",
    "# $ tree /kaggle\n",
    "# kaggle\n",
    "# └── dataset\n",
    "#     └── Nachoneko\n",
    "#         └── 1_gan_cheng\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.png\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.txt\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).png\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).txt\n",
    "#             ├── 0(8).txt\n",
    "#             ├── 0(8).webp\n",
    "#             ├── 001_2.png\n",
    "#             ├── 001_2.txt\n",
    "#             ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.png\n",
    "#             ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.txt\n",
    "#             ├── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.png\n",
    "#             └── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.txt\n",
    "# 4 directories, 12 files\n",
    "# 在填写训练集路径时, 应使用 --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\"\n",
    "# \n",
    "# 模型保存的路径通常用 --output_dir=\"{OUTPUT_PATH}\" 指定, 如 --output_dir=\"{OUTPUT_PATH}/Nachoneko\", OUTPUT_PATH 默认设置为 /kaggle/working/model\n",
    "# 在 Kaggle 的 output 中可以看到保存的模型, 前提是使用 Kaggle 的 Save Version 运行 Kaggle\n",
    "# OUTPUT_PATH 也指定了保存模型到 HuggingFace / ModelScope 的功能的上传路径\n",
    "# \n",
    "# --output_name 用于指定保存的模型名字, 如 --output_name=\"Nachoneko\"\n",
    "# \n",
    "# \n",
    "# 5.\n",
    "# Kaggle 的实例最长可运行 12 h, 要注意训练时长不要超过 12 h, 否则将导致训练被意外中断, 并且最后的模型保存功能将不会得到运行\n",
    "# 如果需要在模型被保存后立即上传到 HuggingFace 进行保存, 可使用启动参数为 sd-scripts 设置自动保存, 具体可阅读 sd-scripts 的帮助信息\n",
    "# 使用 python train_network.py -h 命令可查询可使用的启动参数, 命令中的 train_network.py 可替换成 sdxl_train_network.py 等\n",
    "# \n",
    "# \n",
    "# 6.\n",
    "# 下方提供了一些训练参数, 可以直接使用, 使用时取消注释后根据需求修改部分参数即可\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "#\n",
    "# 这个参数在 animagine-xl-3.1.safetensors 测试, 大概在 30 ~ 40 Epoch 有比较好的效果 (在 36 Epoch 出好效果的概率比较高)\n",
    "#\n",
    "# !accelerate launch \\\n",
    "#     --num_cpu_threads_per_process 1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     sdxl_train_network.py \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/animagine-xl-3.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=1536 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=2 \\\n",
    "#     --max_train_epochs=50 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.0001 \\\n",
    "#     --unet_lr=0.0001 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"cosine_with_restarts\" \\\n",
    "#     --lr_warmup_steps=0 \\\n",
    "#     --lr_scheduler_num_cycles=1 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"tensorboard\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "#\n",
    "# 这个参数是在 Illustrious-XL-v0.1.safetensors 模型上测出来的, 大概在 32 Epoch 左右有比较好的效果\n",
    "# 用 animagine-xl-3.1.safetensors 那套参数也有不错的效果, 只是学习率比这套低了点, 学得慢一点\n",
    "#\n",
    "# !accelerate launch \\\n",
    "#     --num_cpu_threads_per_process 1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     sdxl_train_network.py \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=1536 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=2 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.00012 \\\n",
    "#     --unet_lr=0.00012 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"cosine_with_restarts\" \\\n",
    "#     --lr_warmup_steps=0 \\\n",
    "#     --lr_scheduler_num_cycles=1 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"tensorboard\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "#\n",
    "# 这个参数是在 Illustrious-XL-v0.1.safetensors 模型上测出来的, 大概在 32 Epoch 左右有比较好的效果\n",
    "# 用 animagine-xl-3.1.safetensors 那套参数也有不错的效果, 只是学习率比这套低了点, 学得慢一点\n",
    "# 学习率调度器从 cosine_with_restarts 换成 constant_with_warmup, 此时学习率靠优化器(Lion8bit)进行调度\n",
    "# 拟合速度会更快\n",
    "# constant_with_warmup 用在大规模的训练上比较好, 但用在小规模训练也有不错的效果\n",
    "# \n",
    "# !accelerate launch \\\n",
    "#     --num_cpu_threads_per_process 1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     sdxl_train_network.py \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=1536 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=2 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.00012 \\\n",
    "#     --unet_lr=0.00012 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"constant_with_warmup\" \\\n",
    "#     --lr_warmup_steps=100 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"tensorboard\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "#\n",
    "# 这个参数是在 Illustrious-XL-v0.1.safetensors 模型上测出来的, 大概在 32 Epoch 左右有比较好的效果\n",
    "# 用 animagine-xl-3.1.safetensors 那套参数也有不错的效果, 只是学习率比这套低了点, 学得慢一点\n",
    "# 学习率调度器从 cosine_with_restarts 换成 constant_with_warmup, 此时学习率靠优化器(Lion8bit)进行调度\n",
    "# 拟合速度会更快\n",
    "# constant_with_warmup 用在大规模的训练上比较好, 但用在小规模训练也有不错的效果\n",
    "# \n",
    "# 参数加上了 noise_offset, 可以提高暗处和亮处的表现, 一般使用设置成 0.05 ~ 0.1\n",
    "# 但 noise_offset 可能会导致画面泛白, 光影效果变差\n",
    "# \n",
    "# !accelerate launch \\\n",
    "#     --num_cpu_threads_per_process 1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     sdxl_train_network.py \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=1536 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=2 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.00012 \\\n",
    "#     --unet_lr=0.00012 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"constant_with_warmup\" \\\n",
    "#     --lr_warmup_steps=100 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"tensorboard\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --noise_offset=0.1 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 人物 LoRA, 使用多卡进行训练\n",
    "# 参数中使用了 --scale_weight_norms, 用于提高泛化性, 但可能会造成拟合度降低\n",
    "# 如果当训练人物 LoRA 的图片较多时, 可考虑删去该参数\n",
    "# 当训练人物 LoRA 的图片较少, 为了避免过拟合, 就可以考虑使用 --scale_weight_norms 降低过拟合概率\n",
    "#\n",
    "# !accelerate launch \\\n",
    "#     --num_cpu_threads_per_process 1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     sdxl_train_network.py \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/animagine-xl-3.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/robin\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=1536 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --output_name=\"robin_1\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/robin_1\" \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=2 \\\n",
    "#     --max_train_epochs=50 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --learning_rate=0.0001 \\\n",
    "#     --unet_lr=0.0001 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"cosine_with_restarts\" \\\n",
    "#     --lr_warmup_steps=0 \\\n",
    "#     --lr_scheduler_num_cycles=1 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --scale_weight_norms=1 \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"tensorboard\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 人物 LoRA, 使用多卡进行训练\n",
    "# !accelerate launch \\\n",
    "#     --num_cpu_threads_per_process 1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     sdxl_train_network.py \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/animagine-xl-3.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/murasame_(senren)_3\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=1536 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --output_name=\"murasame_(senren)_10\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/murasame_(senren)_10\" \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=2 \\\n",
    "#     --max_train_epochs=50 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --learning_rate=0.0001 \\\n",
    "#     --unet_lr=0.0001 \\\n",
    "#     --text_encoder_lr=0.00004 \\\n",
    "#     --lr_scheduler=\"cosine_with_restarts\" \\\n",
    "#     --lr_warmup_steps=0 \\\n",
    "#     --lr_scheduler_num_cycles=1 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --scale_weight_norms=1 \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#     --log_with=\"tensorboard\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用单卡进行训练 (Kaggle 的单 Tesla P100 性能不如双 Tesla T4, 建议使用双卡训练)\n",
    "# !python sdxl_train_network.py \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/animagine-xl-3.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/rafa\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=1536 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --output_name=\"rafa_1\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/rafa\" \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=2 \\\n",
    "#     --max_train_epochs=50 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.00007 \\\n",
    "#     --unet_lr=0.00007 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"cosine_with_restarts\" \\\n",
    "#     --lr_warmup_steps=0 \\\n",
    "#     --lr_scheduler_num_cycles=1 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"tensorboard\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --full_fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型上传\n",
    "5. [← 上一个单元](#模型训练)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型上传到 HuggingFace / ModelScope, 通常不需要修改, 修改参数建议通过上方的参数配置单元进行修改\n",
    "if USE_HF_TO_SAVE_MODEL:\n",
    "    echo(\"使用 HuggingFace 保存模型\")\n",
    "    sd_scripts.repo_manager.push_file_to_huggingface(\n",
    "        hf_access_token = HF_TOKEN, # HuggingFace Token\n",
    "        repo = HF_REPO_ID, # HuggingFace 仓库地址\n",
    "        repo_type = HF_REPO_TYPE, # HuggingFace 仓库种类\n",
    "        work_path = os.path.join(WORKSPACE, \"working\", \"MS_REPO\"), # 脚本工作目录, 仓库将克隆至该文件夹中\n",
    "        upload_path = OUTPUT_PATH # 要上传文件的目录\n",
    "    )\n",
    "\n",
    "if USE_MS_TO_SAVE_MODEL:\n",
    "    echo(\"使用 ModelScope 保存模型\")\n",
    "    sd_scripts.repo_manager.set_git_config(GIT_USER_EMAIL, GIT_USER_NAME)\n",
    "    sd_scripts.repo_manager.push_file_to_modelscope(\n",
    "        ms_access_token = MS_TOKEN, # Modelscope Token\n",
    "        repo = MS_REPO_ID, # Modelscope 的仓库地址\n",
    "        work_path = os.path.join(WORKSPACE, \"working\", \"MS_REPO\"), # 脚本工作目录, 仓库将克隆至该文件夹中\n",
    "        upload_path = OUTPUT_PATH # 要上传文件的目录\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
